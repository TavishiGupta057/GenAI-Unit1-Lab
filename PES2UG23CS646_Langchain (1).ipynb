{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**LANCHAIN LAB GEN AI**\n",
        "\n",
        "NAME:TAVISHI GUPTA\n",
        "\n",
        "SRN:PES2UG23CS646\n",
        "\n",
        "SEC:J"
      ],
      "metadata": {
        "id": "RLE8qzSv7Ng8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8m3_5JXntwl2",
        "outputId": "67410864-42b4-4430-e101-a7aa968fbeca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/111.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m500.5/500.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.1/158.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install python-dotenv --upgrade --quiet langchain langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API key: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZ8Io5VAv0ZB",
        "outputId": "74234a5c-568a-47b7-f200-677acbbd4b96"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Google API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PES2UG23CS646\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "#Model A: The \"Accountant\"\n",
        "llm_focused = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", tmeperature=0.0)\n",
        "\n",
        "#Model B: The \"Poet\"\n",
        "llm_creative = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=1.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eASkHLIHwEni",
        "outputId": "9bd01b50-5884-4cb4-f687-4883e16daaa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai.chat_models:Unexpected argument 'tmeperature' provided to ChatGoogleGenerativeAI. Did you mean: 'temperature'?\n",
            "/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:3553: UserWarning: WARNING! tmeperature is not default parameter.\n",
            "                tmeperature was transferred to model_kwargs.\n",
            "                Please confirm that tmeperature is what you intended.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Define the word 'Idea' in one sentence.\"\n",
        "\n",
        "print(\"--- FOCUSED (Temp=0) ---\")\n",
        "print(f\"Run 1: {llm_focused.invoke(prompt).content}\")\n",
        "print(f\"Run 2: {llm_focused.invoke(prompt).content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQJMQnRbwxVR",
        "outputId": "97745fd8-042e-4068-8054-17acafdab394"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- FOCUSED (Temp=0) ---\n",
            "Run 1: An idea is a thought, concept, or mental impression conceived in the mind.\n",
            "Run 2: An idea is a thought, concept, or plan conceived or formed in the mind.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API key: \")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")"
      ],
      "metadata": {
        "id": "EEtQUoxnxQkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "# Scenario: Make AI Rude\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a rude teenage. You use slang and don't care about grammar.\"),\n",
        "    HumanMessage(content=\"What is the capital of the France?\")\n",
        "]\n",
        "\n",
        "response = llm.invoke(messages)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bkUvs2tz1-l",
        "outputId": "37f3b662-b9bb-413e-ab1d-6844cae3d183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ugh, it's Paris. Duh. Like, everyone knows that. Why u even askin, bro? Smh.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a translator. Translate {input_language} to {output_language}.\"),\n",
        "    (\"human\", \"{text}\")\n",
        "])\n",
        "\n",
        "# We can check what inputs it expects\n",
        "print(f\"Required variables: {template.input_variables}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_u9v4AnV0dBf",
        "outputId": "8b5b959f-9320-4cb5-9775-df6b22ccc64e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required variables: ['input_language', 'output_language', 'text']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# Raw Message\n",
        "raw_msg = llm.invoke(\"Hi\")\n",
        "print(f\"Raw Type: {type(raw_msg)}\")\n",
        "\n",
        "# Parsed String\n",
        "clean_text = parser.invoke(raw_msg)\n",
        "print(f\"Parsed Type: {type(clean_text)}\")\n",
        "print(f\"Content: {clean_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_T83HSc0f0M",
        "outputId": "5f65ed41-bd2f-420f-ceb6-6c97bff9e9d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Type: <class 'langchain_core.messages.ai.AIMessage'>\n",
            "Parsed Type: <class 'langchain_core.messages.base.TextAccessor'>\n",
            "Content: Hi there! How can I help you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup (Hidden)\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API Key: \")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
        "template = ChatPromptTemplate.from_template(\"Tell me a fun fact about {topic}.\")\n",
        "parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "sdMSFLO30tNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Format inputs\n",
        "prompt_value = template.invoke({\"topic\": \"Crows\"})\n",
        "\n",
        "# Step 2: Call Model\n",
        "response_obj = llm.invoke(prompt_value)\n",
        "\n",
        "# Step 3: Parse Output\n",
        "final_text = parser.invoke(response_obj)\n",
        "\n",
        "print(final_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xVliu760uoa",
        "outputId": "c67e703b-0531-4939-cfb1-0185dfbd0f21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a fun (and slightly spooky) fact about crows:\n",
            "\n",
            "Crows are incredibly smart, and **they can recognize individual human faces and will remember them for years!** If a crow has a bad experience with a particular person, it's not just that crow that remembers; they'll even teach other crows in their flock to recognize and avoid (or mob!) that person.\n",
            "\n",
            "So, it literally pays to be nice to crows!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the chain once\n",
        "chain = template | llm | parser\n",
        "\n",
        "# Invoke the whole chain\n",
        "print(chain.invoke({\"topic\": \"Octopuses\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfZGD55h0xzD",
        "outputId": "1d57cb85-c698-437c-e631-7927a1f8a9cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Octopuses are super smart! They've been observed solving puzzles, navigating mazes, using tools, and some have even figured out how to unscrew jars or escape from their tanks!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ASSIGNMENT**\n"
      ],
      "metadata": {
        "id": "x4zoeAKx82fq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade langchain langchain-core langchain-google-genai google-generativeai python-dotenv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hF_TEoVz860g",
        "outputId": "1ce67045-a894-4b79-9b02-b84d2e15ff4d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/111.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/500.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m500.5/500.5 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.1/158.1 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n"
      ],
      "metadata": {
        "id": "eMHEScdl9td4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API Key: \")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    temperature=0.0\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqI1qWSy9zNE",
        "outputId": "aa9ed51e-2979-4eb7-ee55-56180950260f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Google API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt Template\n",
        "template = ChatPromptTemplate.from_template(\n",
        "    \"The movie '{movie}' was released in what year? \"\n",
        "    \"Also calculate how many years ago that was from 2026. \"\n",
        "    \"Respond in one clear sentence.\"\n",
        ")\n",
        "\n",
        "# One-line LCEL chain\n",
        "chain = template | llm | StrOutputParser()\n",
        "\n",
        "# Test it\n",
        "print(chain.invoke({\"movie\": \"Inception\"}))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MYFj79394LM",
        "outputId": "77994605-2475-4ccb-bd91-591ed2e74a6f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The movie 'Inception' was released in 2010, which was 16 years ago from 2026.\n"
          ]
        }
      ]
    }
  ]
}