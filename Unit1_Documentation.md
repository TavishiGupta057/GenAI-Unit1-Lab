# Unit 1 â€“ Generative AI Hands-On Documentation

## Key Concepts Understood

- Difference between Encoder-only and Encoder-Decoder Transformer architectures
- Why BERT and RoBERTa are not suitable for text generation tasks
- Importance of model-task alignment in Generative AI
- Role of Masked Language Modeling (MLM) in BERT-based models
- Need for task-specific fine-tuning in Question Answering systems

## Solution Implemented

- Implemented benchmarking experiments using Hugging Face pipelines
- Evaluated BERT, RoBERTa, and BART on:
  - Text Generation
  - Fill-Mask (MLM)
  - Question Answering
- Observed and documented model behavior based on architecture
- Implemented a small prototype mini project using a pre-trained Transformer model
